# InferKVCache

KV Cache ç­–ç•¥å¤ç°
æœ¬é¡¹ç›®æ—¨åœ¨åŸºäº Hugging Face Transformers åŸç”Ÿæ¡†æ¶ï¼Œåœ¨ Llama-2-7Bï¼ˆæˆ– Qwen-7Bï¼‰ä¸Šå¤ç°å¹¶å¯¹æ¯”å¤šç§ KV Cache ä¼˜åŒ–ç­–ç•¥ã€‚æ‰€æœ‰å®éªŒå‡åœ¨ æ—  vLLM / TensorRT-LLM ç­‰é«˜çº§å¼•æ“å¹²æ‰° çš„ç¯å¢ƒä¸‹è¿›è¡Œï¼Œç¡®ä¿å…¬å¹³æ€§ä¸å¯è§£é‡Šæ€§ã€‚
âœ… å®éªŒç›®æ ‡
å¤ç° æœ´ç´  KV Cacheï¼ˆHF åŸç”Ÿ baselineï¼‰
å®ç° Prefix Cachingï¼ˆè·¨è¯·æ±‚å…±äº«å…¬å…±å‰ç¼€ï¼‰
å®ç° CPU Offloadï¼ˆKCache é£æ ¼æ˜¾å­˜å¸è½½ï¼‰
å®ç° Sliding Window Attentionï¼ˆæœ‰é™ä¸Šä¸‹æ–‡çª—å£ï¼‰
ï¼ˆå¯é€‰ï¼‰å®ç° ç®€åŒ–ç‰ˆ PagedAttention
ğŸ“ é¡¹ç›®ç»“æ„

.
â”œâ”€â”€ transformers-5.0.0/        # ä¿®æ”¹åçš„ HF Transformers æºç ï¼ˆå¯ç¼–è¾‘å®‰è£…ï¼‰
â”œâ”€â”€ Llama-2-7b-hf/             # æ¨¡å‹æƒé‡ï¼ˆéœ€æ‰‹åŠ¨ä¸‹è½½ï¼Œå·²åŠ å…¥ .gitignoreï¼‰
â”œâ”€â”€ kv_manager/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # BaseKVCacheManager æ¥å£
â”‚   â”œâ”€â”€ prefix_cache.py        # Prefix Caching å®ç°
â”‚   â”œâ”€â”€ offload_cache.py       # CPU Offload å®ç°
â”‚   â””â”€â”€ sliding_window.py      # Sliding Window å®ç°
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ baseline.py            # Baseline æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ test_prefix.py         # Prefix Caching æµ‹è¯•
â”‚   â””â”€â”€ benchmark.py           # ç»Ÿä¸€æ€§èƒ½è¯„æµ‹
â”œâ”€â”€ results/
â”‚   â””â”€â”€ comparison_table.md    # å®éªŒç»“æœæ±‡æ€»ï¼ˆæ˜¾å­˜ã€ååã€æ­£ç¡®æ€§ï¼‰
â”œâ”€â”€ .gitignore                 # å¿½ç•¥æ¨¡å‹ã€ç¼“å­˜ã€è™šæ‹Ÿç¯å¢ƒç­‰
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
ğŸš€ å¿«é€Ÿå¼€å§‹
1. ç¯å¢ƒå‡†å¤‡


ç¼–è¾‘



# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv kv-cache-env
source kv-cache-env/bin/activate  # Linux/macOS
# kv-cache-env\Scripts\activate  # Windows

# ä¸‹è½½æœ€æ–° HF Transformersï¼ˆæ—  Git å†å²ï¼‰
svn export https://github.com/huggingface/transformers/trunk transformers-dev

# å¯ç¼–è¾‘å®‰è£…
cd transformers-dev && pip install -e ".[dev]" && cd ..
2. ä¸‹è½½æ¨¡å‹ï¼ˆéœ€ Hugging Face è´¦å·ï¼‰
bash

ç¼–è¾‘



huggingface-cli login
huggingface-cli download meta-llama/Llama-2-7b-hf --local-dir ./Llama-2-7b-hf
ğŸ’¡ æˆ–ä½¿ç”¨å…æˆæƒçš„ Qwen-7Bï¼š
python

ç¼–è¾‘



model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
3. è¿è¡Œ Baseline
bash

ç¼–è¾‘



python scripts/baseline.py
4. è¿è¡Œè‡ªå®šä¹‰ç­–ç•¥
bash

ç¼–è¾‘



python scripts/test_prefix.py  # ç¤ºä¾‹ï¼šPrefix Caching
ğŸ“Š è¯„ä¼°æŒ‡æ ‡
æ¯ç§ç­–ç•¥å°†ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„æµ‹ï¼š
è¡¨æ ¼
æŒ‡æ ‡	è¯´æ˜
æ˜¾å­˜å³°å€¼ (GB)	torch.cuda.max_memory_allocated()
ååé‡ (tokens/s)	ç”Ÿæˆ token æ•° / æ€»è€—æ—¶
æ­£ç¡®æ€§	è¾“å‡ºæ˜¯å¦ä¸ baseline ä¸€è‡´ï¼ˆå¯¹ç¡®å®šæ€§ promptï¼‰
æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼Ÿ	èƒ½å¦å¤„ç† >4k tokens çš„è¾“å…¥
è·¨è¯·æ±‚å…±äº«ï¼Ÿ	æ˜¯å¦å…è®¸å¤šä¸ªè¯·æ±‚å¤ç”¨ç›¸åŒå‰ç¼€çš„ KV
ğŸ“ å½“å‰è¿›å±•
è¡¨æ ¼
ç­–ç•¥	çŠ¶æ€	å¤‡æ³¨
Baseline (Naive KV)	âœ… å®Œæˆ	HF åŸç”Ÿ past_key_values
Prefix Caching	â³ å¼€å‘ä¸­	åŸºäº token åºåˆ—å“ˆå¸ŒåŒ¹é…
CPU Offload	ğŸš§ å¾…å¼€å§‹	ä½¿ç”¨ pinned CPU memory
Sliding Window	ğŸš§ å¾…å¼€å§‹	å›ºå®šçª—å£å¤§å° N=2048
PagedAttention (ç®€åŒ–)	âŒ æœªè®¡åˆ’	å¤æ‚åº¦é«˜ï¼Œä¼˜å…ˆçº§ä½
ğŸ“š å‚è€ƒæ–‡çŒ®
vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention
KCache: CPU-GPU Unified KV Cache for Long Context LLM Inference
Mistral 7B Technical Reportï¼ˆSliding Windowï¼‰
Hugging Face Transformers Source Code
ğŸ›‘ æ³¨æ„äº‹é¡¹
ä¸è¦æäº¤æ¨¡å‹æƒé‡ï¼šLlama-2-7b-hf/ å·²åŠ å…¥ .gitignore
ä»…ä¿®æ”¹ transformers-dev/src/transformers/models/llama/modeling_llama.py
æ‰€æœ‰è‡ªå®šä¹‰é€»è¾‘åº”å°è£…åœ¨ kv_manager/ ä¸­ï¼Œä¾¿äºæ›¿æ¢ä¸æµ‹è¯•